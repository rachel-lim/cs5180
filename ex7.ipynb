{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8fb627",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c2a45",
   "metadata": {},
   "source": [
    "# Q3: Solving Four Rooms using semi-gradient SARSA with state aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Four Rooms Environment Implementation\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if reward == 1.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(f\"{fig_title}.png\", dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1118c4",
   "metadata": {},
   "source": [
    "# Q3 - (a): Implement the semi-gradient SARSA\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with a very simple state aggregation strategy. That is aggregating both states and actions to itself. Indeed, this will have similar results as applying SARSA directly. \n",
    "\n",
    "**Please implement the following state aggregation strategy**\n",
    "\n",
    "- For each state, its aggregated state is itself. E.g. [0, 0] is aggregated to [0, 0] only. \n",
    "- For each action, its aggregated action is also itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53298add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SemiGradientSARSAAgent(object):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we fix the function approximation and only discuss different\n",
    "                   state aggregation strategies. Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" will correspond to a unique one-hot representation\n",
    "                                     [0, 0, 0, 1, 0, ..., 0].\n",
    "                   2. Function approximation: we use the linear function approximation. Specifically, the approximation\n",
    "                      function is represented by an N x 1 weight vector, where N = |S| * |A|.\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        # Store the environment\n",
    "        self.env = env\n",
    "\n",
    "        \"\"\" Learning parameters for semi-gradient SARSA \"\"\"\n",
    "        # Store the number of learning episodes\n",
    "        self.episode_num = info['episode_num']\n",
    "\n",
    "        # Store the Q-learning step size alpha\n",
    "        self.alpha = info['alpha']\n",
    "\n",
    "        # Store the discount factor\n",
    "        self.gamma = info['gamma']\n",
    "\n",
    "        # Initialize the epsilon\n",
    "        self.epsilon = info['epsilon']\n",
    "\n",
    "        # Store the hyerparameters\n",
    "        self.params = info\n",
    "\n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We consider a linear function approximation here and store all the weights here\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all states.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|)\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: your state aggregation strategy. Hint: you can start with a simple state aggregation\n",
    "           that just aggregate each state to itself. In other words, the aggregated state space is just\n",
    "           the original state space.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    def _aggregate_action_idx(self, action):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated action.\n",
    "        Args:\n",
    "            action (string): name of the action\n",
    "\n",
    "        To be simple, here, one action only aggregates to itself\n",
    "        \"\"\"\n",
    "        return self.env.action_names.index(action)\n",
    "\n",
    "    def _get_state_action_feature(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the one-hot feature given a state-action pair.\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        # Get the unique index of the aggregated state\n",
    "        state_index = self._aggregate_state_idx(state)\n",
    "        # Get the unique index of the aggregated action\n",
    "        action_index = self._aggregate_action_idx(action)\n",
    "        # Compute the state(aggregated)-action index\n",
    "        state_action_index = self.state_num * action_index + state_index\n",
    "        # Get the one-hot feature of the state\n",
    "        return self.feature_arr[state_action_index]\n",
    "\n",
    "    def function_approximation(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that computes the Q value given a state-action pair using linear function approximation.\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        state_action_feature = self._get_state_action_feature(state, action)\n",
    "        return np.matmul(state_action_feature.T, self.weights_fn)\n",
    "\n",
    "    def render_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the Q value given a state-action pair\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        return self.function_approximation(state, action)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Function implements the epsilon-greedy policy\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: implement the epsilon-greedy policy using function approximation. Break ties if happens \"\"\"\n",
    "        \n",
    "\n",
    "    def update_weights(self, s, a, r, s_prime, a_prime):\n",
    "        \"\"\"\n",
    "        Function that updates the weights using semi-gradients\n",
    "\n",
    "        Args:\n",
    "            s (list): original state\n",
    "            a (string): action name\n",
    "            r (float): reward\n",
    "            s_prime (list): original next state\n",
    "            a_prime (string): next action name\n",
    "        \"\"\"\n",
    "        \"\"\" CODE HERE: implement the update of the semi-gradient SARSA \"\"\"\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        # Save the discounted return for each episode\n",
    "        discounted_returns = []\n",
    "\n",
    "        # Semi-gradient SARSA starts\n",
    "        for ep in tqdm.trange(self.episode_num):\n",
    "            \"\"\"CODE HERE: Implement the pseudocode of the Semi-gradient SARSA\"\"\"\n",
    "            # Reset the agent to initial STATE at the beginning of every episode\n",
    "\n",
    "            # Render an ACTION based on the initial STATE\n",
    "\n",
    "            # Store rewards to compute return G for the current episode.\n",
    "            reward_list = []\n",
    "            \n",
    "            # Loop the episode\n",
    "            for t in range(self.env.max_time_steps):\n",
    "                # Take the ACTION and observe REWARD and NEXT STATE\n",
    "\n",
    "                # Given the NEXT STATE, choose the NEXT ACTION\n",
    "\n",
    "                # Update the weights of the function using semi-gradient SARSA\n",
    "                # Using STATE, ACTION, REWARD, NEXT STATE, NEXT ACTION\n",
    "                \n",
    "                \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "                # Save the reward for plotting\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                # Reset the environment\n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    action = next_action\n",
    "\n",
    "            # compute the discounted return for the current episode\n",
    "            G = 0\n",
    "            for reward in reversed(reward_list):\n",
    "                G = reward + self.gamma * G\n",
    "            discounted_returns.append(G)\n",
    "\n",
    "        return discounted_returns\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1,\n",
    "        'tile_size': 1\n",
    "    }\n",
    "\n",
    "    # set running trials\n",
    "    run_trial = 10\n",
    "\n",
    "    results_1 = []\n",
    "    for _ in range(run_trial):\n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with tile n = 2\n",
    "        tabular_sarsa = SemiGradientSARSAAgent(my_env, params)\n",
    "        res = tabular_sarsa.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_1.append(np.array(res))\n",
    "\n",
    "    plot_curves([np.array(results_1)],\n",
    "                [\"semi-gradient SARSA\"],\n",
    "                [\"b\"],\n",
    "                \"Averaged discounted return\", \"Q3 - (a): semi-gradient SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a745f",
   "metadata": {},
   "source": [
    "# Q3 - (b) [5180]: Implement the semi-gradient SARSA with Tile-based/Room-based aggregation.\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with **Tile-based/Room-based** state aggregation strategy. That is grouping the nearby states in a n x n (i.e., n = 2) tile as one aggregated state. As for the function approximation, we assume to use the same as above.  \n",
    "\n",
    "**Plot**: Plot the learning curves of tile size n = 2 and Room-based aggregation in the same plot. You can use the plot function above to generate the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Tile-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class TileAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: Implement the Tile-based state aggregation here.\n",
    "           Hint: you can manually discretize the original states using the Tile-based method.\n",
    "           For example, you can copy the grid from the Four Rooms environment\n",
    "           and manually aggregate the states (value = 0) in the grid. \n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the aggregated state space using Tile-based method (2x2)\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        \n",
    "        # render the index of the aggregated state\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720faa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Room-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class RoomAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: your state aggregation strategy. Hint: you can start with a simple state aggregation\n",
    "           that just aggregate each state to itself. In other words, the aggregated state space is just\n",
    "           the original state space.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "\n",
    "    # set running trials; You can trial run_trial = 5 to debug\n",
    "    run_trial = 10\n",
    "\n",
    "    # run experiment for the Tile-based method\n",
    "    results_tile = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Tile-based method with tile size n = 2\n",
    "        my_agent = TileAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_tile.append(np.array(res))\n",
    "        \n",
    "    # run experiment for the Room-based method\n",
    "    results_room = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Room-based method\n",
    "        my_agent = RoomAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_room.append(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_curves([np.array(results_1), np.array(results_tile), np.array(results_room)],\n",
    "            [\"State-aggregation: identical\", \"State-aggregation: Tile = 2x2\", \"State aggregation: Room-based\"],\n",
    "            [\"b\", \"r\", \"g\"],\n",
    "            \"Averaged discounted return\", \"Q3 - (b): Comparison between three state aggregation strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a802d",
   "metadata": {},
   "source": [
    "# Q3 - (d):  Adapt your implementation of semi-gradient one-step SARSA for linear function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161caab",
   "metadata": {},
   "source": [
    "# Q3 - (e) [5180]: Implement the following two features, and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b25a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The distance matrix is here\"\"\"\n",
    "\n",
    "distance_matrix = np.array([[14, 13, 12, 11, 10, -1,  4,  3,  2,  1,  0],\n",
    "                            [13, 12, 11, 10,  9, -1,  5,  4,  3,  2,  1],\n",
    "                            [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
    "                            [13, 12, 11, 10,  9, -1,  7,  6,  5,  4,  3],\n",
    "                            [14, 13, 12, 11, 10, -1,  8,  7,  6,  5,  4],\n",
    "                            [-1, 14, -1, -1, -1, -1,  9,  8,  7,  6,  5],\n",
    "                            [16, 15, 16, 17, 18, -1, -1, -1,  8, -1, -1],\n",
    "                            [17, 16, 17, 18, 17, -1, 11, 10,  9, 10, 11],\n",
    "                            [18, 17, 18, 17, 16, -1, 12, 11, 10, 11, 12],\n",
    "                            [19, 18, 17, 16, 15, 14, 13, 12, 11, 12, 13],\n",
    "                            [20, 19, 18, 17, 16, -1, 14, 13, 12, 13, 14]])\n",
    "plt.imshow(distance_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ba1c7-2bed-48e5-a02a-45a6657a6ffe",
   "metadata": {},
   "source": [
    "# Q4: Mountain car.\n",
    "### In this problem, you need to use the mountain car env from gymnasium\n",
    "link: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "\n",
    "Besides, you might also need tiles3.py from https://github.com/kamenbliznashki/sutton_barto/blob/master/tiles3.py to help you implement your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4c5cd-0a09-4988-bd86-45c07ba8ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tiles3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
