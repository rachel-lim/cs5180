{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 and 5 Windy Gridworld Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridWorld(object):\n",
    "    def __init__(self, enable_king_move=False, enable_no_move=False):\n",
    "        # define the grid space\n",
    "        self.grid = np.zeros((7, 10))\n",
    "\n",
    "        # define the state space\n",
    "        self.state_space = [[r, c] for r, c in zip(np.where(self.grid == 0.0)[0],\n",
    "                                                   np.where(self.grid == 0.0)[1])]\n",
    "\n",
    "        # define the start state\n",
    "        self.start_state = [3, 0]\n",
    "\n",
    "        # define the goal state\n",
    "        self.goal_state = [3, 7]\n",
    "\n",
    "        # define the wind\n",
    "        self.wind = np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0], dtype=int)\n",
    "\n",
    "        # # define the action space\n",
    "        # self.action_space = {\n",
    "        #     \"up\": np.array([-1, 0]),\n",
    "        #     \"down\": np.array([1, 0]),\n",
    "        #     \"left\": np.array([0, -1]),\n",
    "        #     \"right\": np.array([0, 1])\n",
    "        # }\n",
    "        \n",
    "        # Enable King's moves (Comment out the above action space to create a new one for King's moves)\n",
    "        if enable_king_move:\n",
    "            \n",
    "            if enable_no_move:\n",
    "                action_space = {\n",
    "                    \"up\": np.array([-1, 0]),\n",
    "                    \"down\": np.array([1, 0]),\n",
    "                    \"left\": np.array([0, -1]),\n",
    "                    \"right\": np.array([0, 1]),\n",
    "                    \"up right\": np.array([-1, 1]),\n",
    "                    \"up left\": np.array([-1, -1]),\n",
    "                    \"down right\": np.array([1, 1]),\n",
    "                    \"down left\": np.array([1, -1]),\n",
    "                    \"none\": np.array([0, 0])\n",
    "                }\n",
    "            else:\n",
    "                action_space = {\n",
    "                    \"up\": np.array([-1, 0]),\n",
    "                    \"down\": np.array([1, 0]),\n",
    "                    \"left\": np.array([0, -1]),\n",
    "                    \"right\": np.array([0, 1]),\n",
    "                    \"up right\": np.array([-1, 1]),\n",
    "                    \"up left\": np.array([-1, -1]),\n",
    "                    \"down right\": np.array([1, 1]),\n",
    "                    \"down left\": np.array([1, -1])\n",
    "                }\n",
    "        \n",
    "        else:\n",
    "            action_space = {\n",
    "                \"up\": np.array([-1, 0]),\n",
    "                \"down\": np.array([1, 0]),\n",
    "                \"left\": np.array([0, -1]),\n",
    "                \"right\": np.array([0, 1])\n",
    "            }\n",
    "            \n",
    "        self.action_space = action_space\n",
    "            \n",
    "                \n",
    "        # track the current state, time step, and action\n",
    "        self.state = None\n",
    "        self.t = None\n",
    "        self.act = None\n",
    "\n",
    "    def reset(self):\n",
    "        # reset the agent to the start state\n",
    "        self.state = self.start_state\n",
    "        # reset the time step tracker\n",
    "        self.t = 0\n",
    "        # reset the action tracker\n",
    "        self.act = None\n",
    "        # reset the terminal flag\n",
    "        terminated = False\n",
    "        return self.state, terminated\n",
    "\n",
    "    def step(self, act):\n",
    "        if act not in self.action_space.keys:\n",
    "            raise ValueError(f\"Invalid action: {act}\")\n",
    "\n",
    "        next_state = self.state + self.wind[self.state[1]] + self.action_space[act]    \n",
    "        \n",
    "        if list(next_state) in self.state_space:\n",
    "            self.state = next_state\n",
    "\n",
    "        if self.state == self.goal_state:\n",
    "            reward = 0\n",
    "            terminated = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminated = False\n",
    "        \n",
    "        return self.state, reward, terminated\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # agent = 1\n",
    "        # goal = 2\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[self.state[0], self.state[1]] = 1.0\n",
    "        plot_arr[self.goal_state[0], self.goal_state[1]] = 2.0\n",
    "        plt.clf()\n",
    "        fig, arr = plt.subplots(1, 1)\n",
    "        arr.set_title(f\"state={self.state}, act={self.act}\")\n",
    "        arr.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(1)\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(l):\n",
    "    return np.random.choice(np.flatnonzero(l == l.max()))\n",
    "\n",
    "    \n",
    "class SARSA(object):\n",
    "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
    "        # define the parameters\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "\n",
    "        # define the Q value table\n",
    "        self.state_num = len(self.env.state_space)\n",
    "        self.action_num = len(self.env.action_space.keys())\n",
    "        self.Q = np.zeros((self.state_num, self.action_num))\n",
    "\n",
    "        # define the timeout\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def behavior_policy(self, state):\n",
    "        r = np.random.rand()\n",
    "\n",
    "        if r < self.epsilon:\n",
    "            return np.random.choice(list(self.env.action_space.keys()))\n",
    "        else:\n",
    "            return argmax(list(self.Q[env.state_space.index(state), :])\n",
    "        return None\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime):\n",
    "        #TODO\n",
    "        return None\n",
    "\n",
    "    def rollout(self):\n",
    "        pass\n",
    "        return None\n",
    "\n",
    "    def run(self):\n",
    "        #TODO\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {'a': 1, 'b': 2}\n",
    "np.random.choice(list(x.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSA(object):\n",
    "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
    "        # define the parameters\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "\n",
    "        # define the Q value table\n",
    "        self.state_num = len(self.env.state_space)\n",
    "        self.action_num = len(self.env.action_space.keys())\n",
    "        self.Q = np.zeros((self.state_num, self.action_num))\n",
    "\n",
    "        # define the timeout\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def behavior_policy(self, state):\n",
    "        #TODO\n",
    "        return None\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime):\n",
    "        #TODO\n",
    "        return None \n",
    "    \n",
    "    def rollout(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        #TODO\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(object):\n",
    "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
    "        # define the parameters\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "\n",
    "        # define the Q value table\n",
    "        self.state_num = len(self.env.state_space)\n",
    "        self.action_num = len(self.env.action_space.keys())\n",
    "        self.Q = np.zeros((self.state_num, self.action_num))\n",
    "\n",
    "        # define the timeout\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def behavior_policy(self, state):\n",
    "        #TODO\n",
    "        return None\n",
    "\n",
    "    def update(self, s, a, r, s_prime):\n",
    "        #TODO\n",
    "        return None\n",
    "\n",
    "    def rollout(self):\n",
    "        pass\n",
    "\n",
    "    def run(self):\n",
    "        #TODO\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "    Make sure the elements in the arr_list, legend_list, and color_list are associated with each other correctly.\n",
    "    Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # Clear the current figure\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # Plot results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # Compute the mean and standard error while ignoring NaN values\n",
    "        mean_arr = np.nanmean(arr, axis=0)\n",
    "        arr_err = np.nanstd(arr, axis=0) / np.sqrt(np.sum(~np.isnan(arr), axis=0))\n",
    "        \n",
    "        # Plot the mean\n",
    "        h, = ax.plot(range(len(mean_arr)), mean_arr, color=color, label=legend)\n",
    "        \n",
    "        # Plot the confidence band\n",
    "        arr_err = 1.96 * arr_err  # 95% confidence interval\n",
    "        ax.fill_between(range(len(mean_arr)),\n",
    "                        mean_arr - arr_err,\n",
    "                        mean_arr + arr_err,\n",
    "                        alpha=0.3, color=color)\n",
    "        # Save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # Set the title (adjust as needed)\n",
    "    ax.set_title(\"Windy Gridworld Results\")\n",
    "    ax.legend(handles=h_list)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_policy_td_control(run_num, timeout):\n",
    "    \n",
    "    enable_king_move_actions = False\n",
    "    enable_no_move_actions = False\n",
    "    \n",
    "    # create the environment\n",
    "    env = WindyGridWorld(enable_king_move=enable_king_move_actions, enable_no_move=enable_no_move_actions)\n",
    "\n",
    "    # parameters\n",
    "    epsilon = 0.1\n",
    "    alpha = 0.5\n",
    "    gamma = 1.0\n",
    "\n",
    "    # create the expected SARSA\n",
    "    expected_sarsa_results_list = []\n",
    "    for _ in range(run_num):\n",
    "        # run for each trial\n",
    "        controller_expected_sarsa = ExpectedSARSA(env, alpha, epsilon, gamma, timeout)\n",
    "        episodes = controller_expected_sarsa.run()\n",
    "        # append the results\n",
    "        expected_sarsa_results_list.append(episodes[0:8000])\n",
    "        \n",
    "    # create the SARSA\n",
    "    sarsa_results_list = []\n",
    "    for _ in range(run_num):\n",
    "        # run for each trial\n",
    "        controller_sarsa = SARSA(env, alpha, epsilon, gamma, timeout)\n",
    "        episodes = controller_sarsa.run()\n",
    "        # append the results\n",
    "        sarsa_results_list.append(episodes[0:8000])\n",
    "\n",
    "    # create the Q learning\n",
    "    q_learning_results_list = []\n",
    "    for _ in range(run_num):\n",
    "        # run for each trial\n",
    "        controller_q_learning = QLearning(env, alpha, epsilon, gamma, timeout)\n",
    "        episodes = controller_q_learning.run()\n",
    "        # append the results\n",
    "        q_learning_results_list.append(episodes[0:8000])\n",
    "    \n",
    "    sarsa_array = np.array(sarsa_results_list)\n",
    "    expected_sarsa_array = np.array(expected_sarsa_results_list)\n",
    "    q_learning_array = np.array(q_learning_results_list)\n",
    "    \n",
    "    # Plot the results\n",
    "    plot_curves(\n",
    "        [sarsa_array, expected_sarsa_array, q_learning_array],\n",
    "        ['SARSA', 'Expected SARSA', 'Q-learning'],\n",
    "        ['r', 'b', 'g'],\n",
    "        \"Episodes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set randomness\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # trial number\n",
    "    trial_num = 10\n",
    "    # maximal time steps\n",
    "    max_time_steps = 8000\n",
    "    \n",
    "    \n",
    "    # run SARSA and Q Learning\n",
    "    run_on_policy_td_control(trial_num, max_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 [5180 ONLY]\n",
    "- Please refer to starter code from question 4 to help you get started. You will create your own TD(0) and Monte Carlo classes.\n",
    "- We will continue with the original windy gridworld domain. \n",
    "- A fixed policy π will be specified to collect episodes.\n",
    "- A certain number of “training” episodes N ∈ {1, 10, 50} will be collected.\n",
    "- Each method being investigated ( On-policy TD(0), On-policy Monte-Carlo prediction) will learn to      estimate the state-value.\n",
    "function using the N “training“ episodes, respectively.\n",
    "- We then evaluate the distribution of learning targets each method experiences at a specified state S. In\n",
    "this question, S is the initial state marked as S in the Example 6.5.\n",
    "- To do so, you need to collect additional 100 “evaluation” episodes. Instead of using these to perform\n",
    "further updates to the state-value function, we will instead evaluate the distribution of learning targets\n",
    "V(S) based on the “evaluation” episodes. For example, TD(0) will experience a set of {R+ V(S′)} targets,\n",
    "whereas Monte-Carlo will experience a set of {G} targets.\n",
    "- Note that in practice you should pre-collect both the training and evaluation episodes for efficiency and to\n",
    "ensure consistency while comparing between different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_policy_mc_td_epsilon_greedy_windy_gridworld():\n",
    "    enable_king_move_actions = False\n",
    "    enable_no_move_actions = False\n",
    "    \n",
    "    # create environments\n",
    "    env = WindyGridWorld(enable_king_move=enable_king_move_actions,\n",
    "                         enable_no_move=enable_no_move_actions)\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set randomness\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # run Monte Carlo and TD(0) for Question 6. Modify as necessary\n",
    "    run_on_policy_mc_td_epsilon_greedy_windy_gridworld()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
